{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "(3 points)\n",
    "\n",
    "Implement the training loop for one training epoch.\n",
    "An epoch trains on the whole training dataset once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, use_cuda, train_loader, optimizer, epoch, log_interval):\n",
    "    \"\"\"\n",
    "    Train one epoch\n",
    "    \n",
    "    model -- the neural network\n",
    "    use_cuda -- true if GPU should be used\n",
    "    train_loader -- data loader\n",
    "    optimizer -- network optimizer\n",
    "    epoch -- number of current epoch\n",
    "    log_interval -- number of training steps between logs\n",
    "    \"\"\"\n",
    "\n",
    "    criterion = F.nll_loss # negative log likelihood loss here\n",
    "\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # TODO: set the model to train mode\n",
    "    model.train()\n",
    "    # TODO: enumerate over the dataloader to get mini batches\n",
    "    #       of images and ground truth labels\n",
    "    # HINT: the builtin python function enumerate() also gives you indices\n",
    "    for batch_idx,item in enumerate(train_loader):\n",
    "        # get the images and the truth labels\n",
    "        images, truth_labels = item\n",
    "        images, truth_labels = images.to(device), truth_labels.to(device)\n",
    "\n",
    "        # TODO: set the optimizers gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # TODO: run the network\n",
    "        outputs = model(images)\n",
    "        # TODO: compute negative log likelihood loss\n",
    "        loss = criterion(outputs,truth_labels)\n",
    "        # TODO: do backpropagation\n",
    "        loss.backward()\n",
    "        # TODO: optimize\n",
    "        optimizer.step() # update the weights\n",
    "\n",
    "        # TODO: print current loss for every nth (\"log_interval\"th) iteration\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(images)}/{len(train_loader.dataset)}'\n",
    "                  f' ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already implemented the validation function for you (this is essentially validate() from the last exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, use_cuda, test_loader):\n",
    "    \"\"\"\n",
    "    Compute test metrics\n",
    "    \n",
    "    model -- the neural network\n",
    "    use_cuda -- true if GPU should be used\n",
    "    test_loader -- data loader\n",
    "    \"\"\"\n",
    "    # create a 10x10 grid of subplots\n",
    "    _, axis = plt.subplots(10, 10)\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    plotted = 0\n",
    "    \n",
    "    # disable gradients globally\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            # for each batch\n",
    "            if use_cuda:\n",
    "                # transfer to GPU\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "            \n",
    "            # run network and compute metrics\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            \n",
    "            img_correct = pred.eq(target.view_as(pred))\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "            # plot the first 100 images\n",
    "            img_idx = 0\n",
    "            data = data.cpu().numpy()\n",
    "            \n",
    "            #while plotted < 100 and img_idx < data.shape[0]:\n",
    "            while plotted < 10 and img_idx < data.shape[0]:# i reduced the number of plots\n",
    "                # compute position of ith image in the grid\n",
    "                y = plotted % 10\n",
    "                x = plotted // 10\n",
    "                \n",
    "                # convert image tensor to numpy array and normalize to [0, 1]\n",
    "                img = data[img_idx, 0]\n",
    "                img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "                \n",
    "                # make wrongly predicted images red\n",
    "                img = np.stack([img] * 3, 2)\n",
    "                if img_correct[img_idx] == 0:\n",
    "                    img[:, :, 1:] = 0.0\n",
    "                \n",
    "                # disable axis and show image\n",
    "                axis[y][x].axis('off')\n",
    "                axis[y][x].imshow(img)\n",
    "                \n",
    "                # show the predicted class next to each image\n",
    "                axis[y][x].text(30, 25, pred[img_idx].item())\n",
    "                \n",
    "                plotted += 1\n",
    "                img_idx += 1\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # show results\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "(4 points)\n",
    "\n",
    "Implement a five-layer fully connected neural network.\n",
    "The dimensions (without batch size) should change like this: 784->200->100->60->30->10\n",
    "Use log softmax to compute the class predictions.\n",
    "\n",
    "Run the code at the end of the notebook to train and validate your implementation.\n",
    "\n",
    "### Task 2.1\n",
    "* sigmoid non-linear activation function\n",
    "* note that the last layer does not need an activation function!\n",
    "\n",
    "### Task 2.2\n",
    "* add a new class \"FCNet2\"\n",
    "* replace sigmoid with ReLU\n",
    "\n",
    "### Task 2.3\n",
    "* add a new class \"FCNet2\"\n",
    "* add batch normalization to the first and third layers (note the difference between 1D/2D/3D versions)\n",
    "\n",
    "\n",
    "**NOTE:** The perfomance should improve slightly with each step. However, due to the random weight initialization applied by PyTorch, your results may vary a bit between trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet1(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully Connected Neural Network\n",
    "    \n",
    "    Five fully connected layers with sigmoid non-linearity\n",
    "    \n",
    "    Dimensions\n",
    "    784->200->100->60->30->10\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_size_list=[784, 200, 100, 60, 30, 10]): # add the dropout probability here as well\n",
    "        super(FCNet1, self).__init__()\n",
    "        self.layer_size_list = layer_size_list\n",
    "        # TODO: initialize network layers\n",
    "        # HINT: take a look at \"torch.nn\" (imported as \"nn\")\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        for i in range(len(self.layer_size_list) - 1):\n",
    "            self.linear_layers.append(\n",
    "                nn.Linear(self.layer_size_list[i], self.layer_size_list[i + 1])\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        # TODO: reshape batch of images to batch of 1D vectors\n",
    "        x = x.view(x.size(0), -1)# This would reshape input from (batch_size, 1, 28, 28) to (batch_size, 784)\n",
    "        \n",
    "        # TODO: run network layers\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            x = torch.sigmoid(layer(x))\n",
    "        # Apply the final layer (no activation)\n",
    "        x = self.linear_layers[-1](x)\n",
    "\n",
    "        # TODO: compute log softmax over the output\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet2(nn.Module):\n",
    "    def __init__(self, layer_size_list=[784, 200, 100, 60, 30, 10]):# add dropout later\n",
    "        super(FCNet2, self).__init__()\n",
    "\n",
    "        self.layer_size_list = layer_size_list\n",
    "        \n",
    "        self.linear_layers = nn.ModuleList() # this is list of linear layer\n",
    "        for i in range(len(self.layer_size_list) - 1):\n",
    "            self.linear_layers.append(\n",
    "                nn.Linear(self.layer_size_list[i], self.layer_size_list[i + 1])\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        # TODO: reshape batch of images to batch of 1D vectors\n",
    "        x = x.view(x.size(0), -1)# This would reshape input from (batch_size, 1, 28, 28) to (batch_size, 784)\n",
    "        \n",
    "        # TODO: run network layers\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        # Apply the final layer (no activation)\n",
    "        x = self.linear_layers[-1](x)\n",
    "\n",
    "        # TODO: compute log softmax over the output\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet3(nn.Module):\n",
    "    def __init__(self, layer_size_list=[784, 200, 100, 60, 30, 10]):\n",
    "\n",
    "        super(FCNet3, self).__init__()\n",
    "\n",
    "        self.layer_size_list = layer_size_list\n",
    "\n",
    "        self.linear_layers = nn.ModuleList() # this is list of linear layer\n",
    "        self.batchnorm_layers = nn.ModuleDict() # list of BN layer\n",
    "        \n",
    "        for i in range(len(self.layer_size_list) - 1):\n",
    "            self.linear_layers.append(\n",
    "                nn.Linear(self.layer_size_list[i], self.layer_size_list[i + 1])\n",
    "            )\n",
    "            if i==0 or i==2:\n",
    "                self.batchnorm_layers[str(i)]=nn.BatchNorm1d(self.layer_size_list[i + 1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: reshape batch of images to batch of 1D vectors\n",
    "        x = x.view(x.size(0), -1)# This would reshape input from (batch_size, 1, 28, 28) to (batch_size, 784)\n",
    "\n",
    "        # TODO: run network layers\n",
    "        for i in range(len(self.linear_layers)-1):\n",
    "            x=self.linear_layers[i](x)\n",
    "            # apply BN layer to 1st and 3rd layer\n",
    "            if i==0 or i==2:\n",
    "                x=self.batchnorm_layers[str(i)](x)\n",
    "            # apply activation function\n",
    "            x = F.relu(x)\n",
    "        # Apply the final layer (no activation)\n",
    "        x = self.linear_layers[-1](x)\n",
    "\n",
    "        # TODO: compute log softmax over the output\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "(3 points)\n",
    "\n",
    "Implement a convolutional neural network, consisting of two convolutional and two fully connected layers.\n",
    "This time, the dimensions (without batch size) should change like this: 1x28x28->32x26x26->64x12x12->128->10\n",
    "\n",
    "### Task 3.1\n",
    "* two convolutional layers (kernel size 3)\n",
    "* two fully-connected layers\n",
    "* ReLU activation function\n",
    "\n",
    "### Task 3.2\n",
    "* add batch normalization to first convolutional and first fully connected layer\n",
    "\n",
    "### Task 3.3\n",
    "* use max pooling instead of stride to reduce the dimensions to 64x12x12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "\n",
    "Note these formulae:\n",
    "\n",
    "1. For a simple kernel and stride based donwsampling:\n",
    "* O = ((W - K + 2P) / S) + 1\n",
    "\n",
    "Where:\n",
    "\n",
    "* O is the output size\n",
    "* W is the input size (width or height)\n",
    "* K is the kernel size\n",
    "* P is the padding\n",
    "* S is the stride\n",
    "\n",
    "2. For maxpooling based downsampling:\n",
    "* O= $\\left \\lfloor\\frac{(W + 2P - Dialation \\times (k-1)-1 )}{S} +1 \\right \\rfloor$\n",
    "\n",
    "Dilation is 1 for maxpooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet1(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network\n",
    "    \n",
    "    Two convolutional layers and two fully connected layers\n",
    "    \n",
    "    Dimensions:\n",
    "    1x28x28->32x26x26->64x12x12->128->10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ConvNet1, self).__init__()\n",
    "        \n",
    "        # TODO: initialize network layers\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)  # 28x28 -> 26x26\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2) # 26x26 -> 12x12\n",
    "\n",
    "        # Flattened size after conv2: 64 * 12 * 12 = 9216\n",
    "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv layers with ReLU\n",
    "        x = F.relu(self.conv1(x))  # 1x28x28 -> 32x26x26\n",
    "        x = F.relu(self.conv2(x))  # 32x26x26 -> 64x24x24\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # batch x 9216\n",
    "\n",
    "        # Fully connected layers with ReLU\n",
    "        x = F.relu(self.fc1(x))    # 9216 -> 128\n",
    "        x = self.fc2(x)            # 128 -> 10\n",
    "\n",
    "        # Log softmax for class probabilities\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet2(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network\n",
    "    \n",
    "    Two convolutional layers and two fully connected layers\n",
    "    Batch normalisation after 1CNN and 1FC layer\n",
    "    \n",
    "    Dimensions:\n",
    "    1x28x28->32x26x26->64x12x12->128->10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ConvNet2, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)   # 28x28 -> 26x26\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # BatchNorm after conv1\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)  # 26x26 -> 12 x 12\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(128)  # BatchNorm after first FC\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv layer 1 + BN + ReLU\n",
    "        x = F.relu(self.bn1(self.conv1(x)))   # 1x28x28 -> 32x26x26\n",
    "        # Conv layer 2 + ReLU\n",
    "        x = F.relu(self.conv2(x))             # 32x26x26 -> 64x12x12\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)             # batch x 9216\n",
    "        # FC layer 1 + BN + ReLU\n",
    "        x = F.relu(self.bn_fc1(self.fc1(x)))  # 9216 -> 128\n",
    "        # FC layer 2\n",
    "        x = self.fc2(x)                       # 128 -> 10\n",
    "        # LogSoftmax\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose k=4,P=0, Dilation=1, and stride =3 for input image of W=26, then :\n",
    "\n",
    "O=((26+0-4)/2)+1 => O = 11 + 1 = 12!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet3, self).__init__()\n",
    "        \n",
    "        # TODO: initialize network layers\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3)  # 28x28 -> 26x26\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=4, stride=2) # 26x26 -> 12x12\n",
    "\n",
    "        # Flattened size after conv2: 64 * 12 * 12 = 9216\n",
    "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv layers with ReLU\n",
    "        x = F.relu(self.conv1(x))  # 1x28x28 -> 32x26x26\n",
    "        x = F.relu(self.maxpool1(x))  # 32x26x26 -> 64x24x24\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # batch x 9216\n",
    "\n",
    "        # Fully connected layers with ReLU\n",
    "        x = F.relu(self.fc1(x))    # 9216 -> 128\n",
    "        x = self.fc2(x)            # 128 -> 10\n",
    "\n",
    "        # Log softmax for class probabilities\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 2#10\n",
    "lr = 1.0\n",
    "gamma = 0.7\n",
    "log_interval = 100\n",
    "\n",
    "# use GPU if available\n",
    "use_cuda = False#torch.cuda.is_available()\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# initialize data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])), batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, IDK why when I keep `validate()` function, the kernel keeps on crashing on my laptop, thus meanwhile I have removed the validation function in the code below. (note that this code was in already present in the code, in the cell above, so we cannot simply comment `validate` function in the final solution.).\n",
    "\n",
    "\n",
    "I ran script in google colab and it worked quite well. try it in your systems as well. if everything works well, then change the number of epochs to 10 in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCNet1()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# initialize optimizer and scheduler\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in trange(1, epochs + 1, desc=\"Epochs\"):\n",
    "    # train one epoch\n",
    "    train(model, use_cuda, train_loader, optimizer, epoch, log_interval)\n",
    "    \n",
    "    # run on test dataset\n",
    "    #validate(model, use_cuda, test_loader)\n",
    "    # I think there is some problem in the validate model\n",
    "    scheduler.step()\n",
    "    \n",
    "    #torch.save(model.state_dict(), \"models/mnist/checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCNet2()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# initialize optimizer and scheduler\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # train one epoch\n",
    "    train(model, use_cuda, train_loader, optimizer, epoch, log_interval)\n",
    "    \n",
    "    # run on test dataset\n",
    "    # validate(model, use_cuda, test_loader) \n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "    #torch.save(model.state_dict(), \"models/mnist/checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCNet3()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# initialize optimizer and scheduler\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # train one epoch\n",
    "    train(model, use_cuda, train_loader, optimizer, epoch, log_interval)\n",
    "    \n",
    "    # run on test dataset\n",
    "    # validate(model, use_cuda, test_loader)\n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "    #torch.save(model.state_dict(), \"models/mnist/checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running CNN networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet1()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# initialize optimizer and scheduler\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in trange(1, epochs + 1, desc=\"Epochs\"):\n",
    "    # train one epoch\n",
    "    train(model, use_cuda, train_loader, optimizer, epoch, log_interval)\n",
    "    \n",
    "    # run on test dataset\n",
    "    #validate(model, use_cuda, test_loader)\n",
    "    # I think there is some problem in the validate model\n",
    "    scheduler.step()\n",
    "    \n",
    "    #torch.save(model.state_dict(), \"models/mnist/checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet2()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# initialize optimizer and scheduler\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in trange(1, epochs + 1, desc=\"Epochs\"):\n",
    "    # train one epoch\n",
    "    train(model, use_cuda, train_loader, optimizer, epoch, log_interval)\n",
    "    \n",
    "    # run on test dataset\n",
    "    #validate(model, use_cuda, test_loader)\n",
    "    # I think there is some problem in the validate model\n",
    "    scheduler.step()\n",
    "    \n",
    "    #torch.save(model.state_dict(), \"models/mnist/checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet3()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# initialize optimizer and scheduler\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in trange(1, epochs + 1, desc=\"Epochs\"):\n",
    "    # train one epoch\n",
    "    train(model, use_cuda, train_loader, optimizer, epoch, log_interval)\n",
    "    \n",
    "    # run on test dataset\n",
    "    #validate(model, use_cuda, test_loader)\n",
    "    # I think there is some problem in the validate model\n",
    "    scheduler.step()\n",
    "    \n",
    "    #torch.save(model.state_dict(), \"models/mnist/checkpoint.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
