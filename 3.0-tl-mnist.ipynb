{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "(3 points)\n",
    "\n",
    "Implement the training loop for one training epoch.\n",
    "An epoch trains on the whole training dataset once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, use_cuda, train_loader, optimizer, epoch, log_interval):\n",
    "    \"\"\"\n",
    "    Train one epoch\n",
    "    \n",
    "    model -- the neural network\n",
    "    use_cuda -- true if GPU should be used\n",
    "    train_loader -- data loader\n",
    "    optimizer -- network optimizer\n",
    "    epoch -- number of current epoch\n",
    "    log_interval -- number of training steps between logs\n",
    "    \"\"\"\n",
    "\n",
    "    criterion = F.nll_loss # negative log likelihood loss here\n",
    "\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # TODO: set the model to train mode\n",
    "    model.train()\n",
    "    # TODO: enumerate over the dataloader to get mini batches\n",
    "    #       of images and ground truth labels\n",
    "    # HINT: the builtin python function enumerate() also gives you indices\n",
    "    for batch_idx,item in enumerate(train_loader):\n",
    "        # get the images and the truth labels\n",
    "        images, truth_labels = item\n",
    "        images, truth_labels = images.to(device), truth_labels.to(device)\n",
    "\n",
    "        # TODO: set the optimizers gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # TODO: run the network\n",
    "        outputs = model(images)\n",
    "        # TODO: compute negative log likelihood loss\n",
    "        loss = criterion(outputs,truth_labels)\n",
    "        # TODO: do backpropagation\n",
    "        loss.backward()\n",
    "        # TODO: optimize\n",
    "        optimizer.step() # update the weights\n",
    "\n",
    "        # TODO: print current loss for every nth (\"log_interval\"th) iteration\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(images)}/{len(train_loader.dataset)}'\n",
    "                  f' ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already implemented the validation function for you (this is essentially validate() from the last exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, use_cuda, test_loader):\n",
    "    \"\"\"\n",
    "    Compute test metrics\n",
    "    \n",
    "    model -- the neural network\n",
    "    use_cuda -- true if GPU should be used\n",
    "    test_loader -- data loader\n",
    "    \"\"\"\n",
    "    # create a 10x10 grid of subplots\n",
    "    _, axis = plt.subplots(10, 10)\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    plotted = 0\n",
    "    \n",
    "    # disable gradients globally\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            # for each batch\n",
    "            if use_cuda:\n",
    "                # transfer to GPU\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "            \n",
    "            # run network and compute metrics\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            \n",
    "            img_correct = pred.eq(target.view_as(pred))\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "            # plot the first 100 images\n",
    "            img_idx = 0\n",
    "            data = data.cpu().numpy()\n",
    "            \n",
    "            #while plotted < 100 and img_idx < data.shape[0]:\n",
    "            while plotted < 10 and img_idx < data.shape[0]:# i reduced the number of plots\n",
    "                # compute position of ith image in the grid\n",
    "                y = plotted % 10\n",
    "                x = plotted // 10\n",
    "                \n",
    "                # convert image tensor to numpy array and normalize to [0, 1]\n",
    "                img = data[img_idx, 0]\n",
    "                img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "                \n",
    "                # make wrongly predicted images red\n",
    "                img = np.stack([img] * 3, 2)\n",
    "                if img_correct[img_idx] == 0:\n",
    "                    img[:, :, 1:] = 0.0\n",
    "                \n",
    "                # disable axis and show image\n",
    "                axis[y][x].axis('off')\n",
    "                axis[y][x].imshow(img)\n",
    "                \n",
    "                # show the predicted class next to each image\n",
    "                axis[y][x].text(30, 25, pred[img_idx].item())\n",
    "                \n",
    "                plotted += 1\n",
    "                img_idx += 1\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # show results\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    print(\"Does it work perfectly fine till this point\")\n",
    "    plt.show()\n",
    "    print(\" is the problem somewhere here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "(4 points)\n",
    "\n",
    "Implement a five-layer fully connected neural network.\n",
    "The dimensions (without batch size) should change like this: 784->200->100->60->30->10\n",
    "Use log softmax to compute the class predictions.\n",
    "\n",
    "Run the code at the end of the notebook to train and validate your implementation.\n",
    "\n",
    "### Task 2.1\n",
    "* sigmoid non-linear activation function\n",
    "* note that the last layer does not need an activation function!\n",
    "\n",
    "### Task 2.2\n",
    "* add a new class \"FCNet2\"\n",
    "* replace sigmoid with ReLU\n",
    "\n",
    "### Task 2.3\n",
    "* add a new class \"FCNet2\"\n",
    "* add batch normalization to the first and third layers (note the difference between 1D/2D/3D versions)\n",
    "\n",
    "\n",
    "**NOTE:** The perfomance should improve slightly with each step. However, due to the random weight initialization applied by PyTorch, your results may vary a bit between trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet1(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully Connected Neural Network\n",
    "    \n",
    "    Five fully connected layers with sigmoid non-linearity\n",
    "    \n",
    "    Dimensions\n",
    "    784->200->100->60->30->10\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_size_list=[784, 200, 100, 60, 30, 10]): # add the dropout probability here as well\n",
    "        super(FCNet1, self).__init__()\n",
    "        self.layer_size_list = layer_size_list\n",
    "        # TODO: initialize network layers\n",
    "        # HINT: take a look at \"torch.nn\" (imported as \"nn\")\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        for i in range(len(self.layer_size_list) - 1):\n",
    "            self.linear_layers.append(\n",
    "                nn.Linear(self.layer_size_list[i], self.layer_size_list[i + 1])\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        # TODO: reshape batch of images to batch of 1D vectors\n",
    "        x = x.view(x.size(0), -1)# This would reshape input from (batch_size, 1, 28, 28) to (batch_size, 784)\n",
    "        \n",
    "        # TODO: run network layers\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            x = torch.sigmoid(layer(x))\n",
    "        # Apply the final layer (no activation)\n",
    "        x = self.linear_layers[-1](x)\n",
    "\n",
    "        # TODO: compute log softmax over the output\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet2(nn.Module):\n",
    "    def __init__(self, layer_size_list=[784, 200, 100, 60, 30, 10]):# add dropout later\n",
    "        super(FCNet2, self).__init__()\n",
    "\n",
    "        self.layer_size_list = layer_size_list\n",
    "        \n",
    "        self.linear_layers = nn.ModuleList() # this is list of linear layer\n",
    "        for i in range(len(self.layer_size_list) - 1):\n",
    "            self.linear_layers.append(\n",
    "                nn.Linear(self.layer_size_list[i], self.layer_size_list[i + 1])\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        # TODO: reshape batch of images to batch of 1D vectors\n",
    "        x = x.view(x.size(0), -1)# This would reshape input from (batch_size, 1, 28, 28) to (batch_size, 784)\n",
    "        \n",
    "        # TODO: run network layers\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        # Apply the final layer (no activation)\n",
    "        x = self.linear_layers[-1](x)\n",
    "\n",
    "        # TODO: compute log softmax over the output\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet3(nn.Module):\n",
    "    def __init__(self, layer_size_list=[784, 200, 100, 60, 30, 10]):\n",
    "\n",
    "        super(FCNet3, self).__init__()\n",
    "\n",
    "        self.layer_size_list = layer_size_list\n",
    "\n",
    "        self.linear_layers = nn.ModuleList() # this is list of linear layer\n",
    "        self.batchnorm_layers = nn.ModuleDict() # list of BN layer\n",
    "        \n",
    "        for i in range(len(self.layer_size_list) - 1):\n",
    "            self.linear_layers.append(\n",
    "                nn.Linear(self.layer_size_list[i], self.layer_size_list[i + 1])\n",
    "            )\n",
    "            if i==0 or i==2:\n",
    "                self.batchnorm_layers[str(i)]=nn.BatchNorm1d(self.layer_size_list[i + 1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: reshape batch of images to batch of 1D vectors\n",
    "        x = x.view(x.size(0), -1)# This would reshape input from (batch_size, 1, 28, 28) to (batch_size, 784)\n",
    "\n",
    "        # TODO: run network layers\n",
    "        for i in range(len(self.linear_layers)-1):\n",
    "            x=self.linear_layers[i](x)\n",
    "            # apply BN layer to 1st and 3rd layer\n",
    "            if i==0 or i==2:\n",
    "                x=self.batchnorm_layers[str(i)](x)\n",
    "            # apply activation function\n",
    "            x = F.relu(x)\n",
    "        # Apply the final layer (no activation)\n",
    "        x = self.linear_layers[-1](x)\n",
    "\n",
    "        # TODO: compute log softmax over the output\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "(3 points)\n",
    "\n",
    "Implement a convolutional neural network, consisting of two convolutional and two fully connected layers.\n",
    "This time, the dimensions (without batch size) should change like this: 1x28x28->32x26x26->64x12x12->128->10\n",
    "\n",
    "### Task 3.1\n",
    "* two convolutional layers (kernel size 3)\n",
    "* two fully-connected layers\n",
    "* ReLU activation function\n",
    "\n",
    "### Task 3.2\n",
    "* add batch normalization to first convolutional and first fully connected layer\n",
    "\n",
    "### Task 3.3\n",
    "* use max pooling instead of stride to reduce the dimensions to 64x12x12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet1(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network\n",
    "    \n",
    "    Two convolutional layers and two fully connected layers\n",
    "    \n",
    "    Dimensions:\n",
    "    1x28x28->32x26x26->64x12x12->128->10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ConvNet1, self).__init__()\n",
    "        \n",
    "        # TODO: initialize network layers\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)  # 28x28 -> 26x26\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3) # 26x26 -> 24x24\n",
    "\n",
    "        # Flattened size after conv2: 64 * 24 * 24 = 36864\n",
    "        self.fc1 = nn.Linear(64 * 24 * 24, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv layers with ReLU\n",
    "        x = F.relu(self.conv1(x))  # 1x28x28 -> 32x26x26\n",
    "        x = F.relu(self.conv2(x))  # 32x26x26 -> 64x24x24\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # batch x 36864\n",
    "\n",
    "        # Fully connected layers with ReLU\n",
    "        x = F.relu(self.fc1(x))    # 36864 -> 128\n",
    "        x = self.fc2(x)            # 128 -> 10\n",
    "\n",
    "        # Log softmax for class probabilities\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet2(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network\n",
    "    \n",
    "    Two convolutional layers and two fully connected layers\n",
    "    Batch normalisation after 1CNN and 1FC layer\n",
    "    \n",
    "    Dimensions:\n",
    "    1x28x28->32x26x26->64x12x12->128->10\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ConvNet2, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)   # 28x28 -> 26x26\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # BatchNorm after conv1\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)  # 26x26 -> 24x24\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 24 * 24, 128)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(128)  # BatchNorm after first FC\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv layer 1 + BN + ReLU\n",
    "        x = F.relu(self.bn1(self.conv1(x)))   # 1x28x28 -> 32x26x26\n",
    "        # Conv layer 2 + ReLU\n",
    "        x = F.relu(self.conv2(x))             # 32x26x26 -> 64x24x24\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)             # batch x 36864\n",
    "        # FC layer 1 + BN + ReLU\n",
    "        x = F.relu(self.bn_fc1(self.fc1(x)))  # 36864 -> 128\n",
    "        # FC layer 2\n",
    "        x = self.fc2(x)                       # 128 -> 10\n",
    "        # LogSoftmax\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet3, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 2#10\n",
    "lr = 1.0\n",
    "gamma = 0.7\n",
    "log_interval = 100\n",
    "\n",
    "# use GPU if available\n",
    "use_cuda = False#torch.cuda.is_available()\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# initialize data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])), batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, IDK why when I keep `validate()` function, the kernel keeps on crashing on my laptop, thus meanwhile I have removed the validation function in the code below. (note that this code was in already present in the code, in the cell above, so we cannot simply comment `validate` function in the final solution.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.365259\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.314620\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.302558\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.315111\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.295861\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.292243\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.325281\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.289016\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.300503\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.994141\n",
      "\n",
      "Test set: Average loss: 1.8143, Accuracy: 2223/10000 (22.23%)\n",
      "\n",
      "Does it work perfectly fine till this point\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = FCNet1()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# initialize optimizer and scheduler\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in trange(1, epochs + 1, desc=\"Epochs\"):\n",
    "    # train one epoch\n",
    "    train(model, use_cuda, train_loader, optimizer, epoch, log_interval)\n",
    "    \n",
    "    # run on test dataset\n",
    "    validate(model, use_cuda, test_loader)\n",
    "    # I think there is some problem in the validate model\n",
    "    scheduler.step()\n",
    "    \n",
    "    #torch.save(model.state_dict(), \"models/mnist/checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.312175\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.377308\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.219918\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.090426\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.286938\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.086095\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.195589\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.296510\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.097019\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.090117\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.211579\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.128039\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.178205\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.144463\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.159606\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.182984\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.017491\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.028740\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.196418\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.043180\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.087203\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.049446\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.036499\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.127335\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.110250\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.015532\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.020059\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.030641\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.264694\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.036441\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.044593\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.007705\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.004534\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.000531\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.029649\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.056458\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.165197\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.021297\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.069818\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.096704\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.003397\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.101537\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.001522\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.007627\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.000895\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.026751\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.000487\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.000764\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.001888\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.034144\n"
     ]
    }
   ],
   "source": [
    "model = FCNet2()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# initialize optimizer and scheduler\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # train one epoch\n",
    "    train(model, use_cuda, train_loader, optimizer, epoch, log_interval)\n",
    "    \n",
    "    # run on test dataset\n",
    "    # validate(model, use_cuda, test_loader) \n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "    #torch.save(model.state_dict(), \"models/mnist/checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299743\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.196098\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.386714\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.313938\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.174311\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.234149\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.187599\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.060387\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.031403\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.180847\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.083403\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.030506\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.185305\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.217192\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.077417\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.048841\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.106582\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.030813\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.015723\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.037259\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.033122\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.020288\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.036848\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.075869\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.225623\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.007689\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.023930\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.202193\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.016255\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.008791\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.059848\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.012246\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.064374\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.094118\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.013323\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.028111\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.112962\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.007062\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.032378\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.050768\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.015395\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.010376\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.055779\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.105212\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.003579\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.000657\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.064345\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.078605\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.037934\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.104449\n"
     ]
    }
   ],
   "source": [
    "model = FCNet3()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# initialize optimizer and scheduler\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # train one epoch\n",
    "    train(model, use_cuda, train_loader, optimizer, epoch, log_interval)\n",
    "    \n",
    "    # run on test dataset\n",
    "    # validate(model, use_cuda, test_loader)\n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "    #torch.save(model.state_dict(), \"models/mnist/checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the CNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307529\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.159634\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.071590\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.043399\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.004755\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.030686\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.072325\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.018905\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.076791\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.019082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 1/5 [04:00<16:01, 240.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.027749\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.023949\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.008759\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.006387\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.003703\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.064060\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.005642\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.013176\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.010310\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.030749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 2/5 [08:02<12:04, 241.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.002592\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.005861\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.000110\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.002120\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.017815\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.000703\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.000174\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.008833\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.000280\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.002860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 3/5 [12:06<08:05, 242.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.007639\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.004234\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.000111\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.000269\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.001611\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.000666\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.097966\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.000897\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.000751\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.000944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 4/5 [16:04<04:00, 240.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.000025\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.000541\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.000760\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.043959\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.000812\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.000428\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.000218\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.000016\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.000080\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.000190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 5/5 [20:15<00:00, 243.04s/it]\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet1()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# initialize optimizer and scheduler\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in trange(1, epochs + 1, desc=\"Epochs\"):\n",
    "    # train one epoch\n",
    "    train(model, use_cuda, train_loader, optimizer, epoch, log_interval)\n",
    "    \n",
    "    # run on test dataset\n",
    "    #validate(model, use_cuda, test_loader)\n",
    "    # I think there is some problem in the validate model\n",
    "    scheduler.step()\n",
    "    \n",
    "    #torch.save(model.state_dict(), \"models/mnist/checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.458563\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.076713\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.059905\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.106190\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.134752\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.110792\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.066470\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.024970\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.062919\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.055440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 1/5 [03:39<14:36, 219.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.016042\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.014723\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.069353\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.018531\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.172360\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.010169\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.012620\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.077748\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.039668\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.005474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 2/5 [07:14<10:50, 216.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.015450\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.003647\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.006991\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.000773\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.004994\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.001516\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.005592\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.001873\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.002675\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.016262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 3/5 [10:42<07:05, 212.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.000889\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.004210\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.000577\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.057498\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.038408\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.004375\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.006013\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.002776\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.001744\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.004646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 4/5 [14:13<03:32, 212.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.000812\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.000512\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.002137\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.004673\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.001835\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.000662\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.000350\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.014186\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.004614\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.018354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 5/5 [17:43<00:00, 212.65s/it]\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet2()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# initialize optimizer and scheduler\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in trange(1, epochs + 1, desc=\"Epochs\"):\n",
    "    # train one epoch\n",
    "    train(model, use_cuda, train_loader, optimizer, epoch, log_interval)\n",
    "    \n",
    "    # run on test dataset\n",
    "    #validate(model, use_cuda, test_loader)\n",
    "    # I think there is some problem in the validate model\n",
    "    scheduler.step()\n",
    "    \n",
    "    #torch.save(model.state_dict(), \"models/mnist/checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet3()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# initialize optimizer and scheduler\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "for epoch in trange(1, epochs + 1, desc=\"Epochs\"):\n",
    "    # train one epoch\n",
    "    train(model, use_cuda, train_loader, optimizer, epoch, log_interval)\n",
    "    \n",
    "    # run on test dataset\n",
    "    #validate(model, use_cuda, test_loader)\n",
    "    # I think there is some problem in the validate model\n",
    "    scheduler.step()\n",
    "    \n",
    "    #torch.save(model.state_dict(), \"models/mnist/checkpoint.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
